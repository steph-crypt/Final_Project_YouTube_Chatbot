{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befed352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #downloads youtube video low quality\n",
    "# import yt_dlp\n",
    "\n",
    "# def download_low_quality_videos(playlist_url, max_videos=15, output_dir='downloads'):\n",
    "#     ydl_opts = {\n",
    "#         'outtmpl': f'{output_dir}/%(playlist_index)s - %(title)s.%(ext)s',\n",
    "#         'playlistend': max_videos,\n",
    "#         'format': 'worstvideo+worstaudio/worst',\n",
    "#         'merge_output_format': 'mp4',  # Ensures final file is mp4\n",
    "#         'writesubtitles': True,\n",
    "#         'writeautomaticsub': True,\n",
    "#         'subtitleslangs': ['en'],\n",
    "#         'ignoreerrors': True, #skips unavailable videos so as not to interrupt the downloading\n",
    "#     }\n",
    "\n",
    "#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#         ydl.download([playlist_url])\n",
    "\n",
    "# # Run the function\n",
    "# playlist_url = \"https://www.youtube.com/playlist?list=PLnaXrumrax3X8_6L1yL3cejSMH9oTpxiI\"\n",
    "# download_low_quality_videos(playlist_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yt_dlp\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# def download_low_quality_videos_with_metadata(playlist_url, max_videos=15, output_dir='downloads'):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     metadata_list = []\n",
    "\n",
    "# #gets metadata from video\n",
    "#     def metadata_hook(d):\n",
    "#         if d['status'] == 'finished':\n",
    "#             info = {\n",
    "#                 'id': d.get('id'),\n",
    "#                 'title': d.get('title'),\n",
    "#                 'filename': d.get('filename'),\n",
    "#                 'duration': d.get('duration'),\n",
    "#                 'upload_date': d.get('upload_date'),\n",
    "#                 'uploader': d.get('uploader'),\n",
    "#                 'description': d.get('description'),\n",
    "#                 'webpage_url': d.get('webpage_url'),\n",
    "#             }\n",
    "#             metadata_list.append(info)\n",
    "\n",
    "# #downloads youtube video low quality\n",
    "#     ydl_opts = {\n",
    "#         'outtmpl': f'{output_dir}/%(playlist_index)s - %(title)s.%(ext)s',\n",
    "#         'playlistend': max_videos,\n",
    "#         'format': 'worstvideo+worstaudio/worst',\n",
    "#         'merge_output_format': 'mp4',\n",
    "#         'writesubtitles': True,\n",
    "#         'writeautomaticsub': True,\n",
    "#         'subtitleslangs': ['en'],\n",
    "#         'progress_hooks': [metadata_hook],\n",
    "#         'quiet': False,\n",
    "#         'ignoreerrors': True, #skips unavailable videos so as not to interrupt the downloading\n",
    "\n",
    "#     }\n",
    "\n",
    "#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "#         ydl.download([playlist_url])\n",
    "\n",
    "#     # Save all metadata\n",
    "#     with open(os.path.join(output_dir, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
    "#         json.dump(metadata_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # Run the function\n",
    "# playlist_url = \"https://www.youtube.com/playlist?list=PLnaXrumrax3X8_6L1yL3cejSMH9oTpxiI\"\n",
    "# download_low_quality_videos_with_metadata(playlist_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import whisper\n",
    "\n",
    "# #whisper LLM transcription of audio\n",
    "# def transcribe_with_whisper(video_dir='downloads', output_dir='transcripts'):\n",
    "#     #make directory to save transcription in unless the file already exists\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     #loads base version of the whisper model\n",
    "#     model = whisper.load_model(\"base\")  # You can use \"small\", \"medium\", or \"large\" for higher accuracy\n",
    "\n",
    "# #iterate through list of videos to transcribe\n",
    "#     for filename in os.listdir(video_dir):\n",
    "#         if filename.endswith(\".mp4\"):\n",
    "#             #constructs full path to video file\n",
    "#             filepath = os.path.join(video_dir, filename)\n",
    "#             print(f\"Transcribing: {filepath}\")\n",
    "#             #result contains the transcription and metadata\n",
    "\n",
    "#             # Transcribe audio to text (force language to English)\n",
    "#             result = model.transcribe(filepath, language='en')           \n",
    "#              #text extracts just the plain transcribed text\n",
    "#             text = result['text']\n",
    "\n",
    "#             # Save plain text\n",
    "#             #creates a .txt file for the transcript (same name as video, with .txt extension)\n",
    "#             transcript_txt = os.path.join(output_dir, filename.replace(\".mp4\", \".txt\"))\n",
    "#             with open(transcript_txt, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(text)\n",
    "\n",
    "#             # Creates a .json file with the full transcription data (including segments timestamps, confidence scores)\n",
    "#             # ensure_ascii=False keeps Unicode characters readable (e.g., emojis, accents)\n",
    "#             transcript_json = os.path.join(output_dir, filename.replace(\".mp4\", \".json\"))\n",
    "#             with open(transcript_json, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # calls the function\n",
    "# transcribe_with_whisper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create timestamps for transcripts (to help with chunking the data and allowing the user to find their topic in the original video if needed)\n",
    "\n",
    "# #Converts a timestamp in seconds (float) to SRT format: HH:MM:SS,mmm\n",
    "# def seconds_to_srt_time(seconds):\n",
    "#     hours = int(seconds // 3600)\n",
    "#     minutes = int((seconds % 3600) // 60)\n",
    "#     secs = int(seconds % 60)\n",
    "#     millis = int((seconds - int(seconds)) * 1000)\n",
    "#     return f\"{hours:02}:{minutes:02}:{secs:02},{millis:03}\"\n",
    "\n",
    "# #looks for .json files inside the transcripts/ directory\n",
    "# def generate_srt_from_json(json_dir='transcripts'):\n",
    "#     for filename in os.listdir(json_dir):\n",
    "#         if filename.endswith(\".json\"):\n",
    "#             json_path = os.path.join(json_dir, filename)\n",
    "#             srt_path = json_path.replace(\".json\", \".srt\")\n",
    "\n",
    "#             with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#                 data = json.load(f)\n",
    "\n",
    "#             #Whisper stores transcript in data['segments'] (each segment has start, end, text).\n",
    "#             #If missing, skip and warn.\n",
    "#             if 'segments' not in data:\n",
    "#                 print(f\"⚠️ No segments in {filename}, can't generate SRT.\")\n",
    "#                 continue\n",
    "#             #Creates a new .srt file for writing\n",
    "#             with open(srt_path, 'w', encoding='utf-8') as f:\n",
    "#                 #Loops through all segments and adds SRT-style numbering\n",
    "#                 for i, seg in enumerate(data['segments'], 1):\n",
    "#                     #Converts start and end from float to SRT timestamp strings.\n",
    "#                     start = seconds_to_srt_time(seg['start'])\n",
    "#                     end = seconds_to_srt_time(seg['end'])\n",
    "#                     #Strips extra whitespace from the text.\n",
    "#                     text = seg['text'].strip()\n",
    "#                     f.write(f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "#             print(f\"✅ Saved: {srt_path}\")\n",
    "\n",
    "# #Calls the function to start processing all .json files in transcripts/\n",
    "# generate_srt_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b980b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def text_preprocessing_pipeline(text):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a text string by applying standard NLP cleaning steps:\n",
    "#     tokenization, stop word removal, punctuation removal, and lemmatization.\n",
    "\n",
    "#     Parameters:\n",
    "#         text (str): The input text string to preprocess.\n",
    "\n",
    "#     Returns:\n",
    "#         str: A cleaned and lemmatized string with tokens joined by spaces.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Remove HTML\n",
    "#     text_without_html = re.sub(r'<[^<>]*>', '', text)\n",
    "\n",
    "#     # Tokenize the text\n",
    "#     tokenized_text = word_tokenize(text_without_html.lower())\n",
    "\n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     filtered_tokens = [w for w in tokenized_text if w.lower() not in stop_words]\n",
    "\n",
    "#     # Remove punctuation\n",
    "#     filtered_tokens = [w for w in filtered_tokens if w not in string.punctuation]\n",
    "\n",
    "#     # Apply lemmatization\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "\n",
    "#     return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# csv_data[\"title_text_processed\"] = csv_data[\"title_text\"].apply(text_preprocessing_pipeline)\n",
    "# csv_data[\"combined_text_processed\"] = csv_data[\"combined_text\"].apply(text_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the First N Segments of a Transcript\n",
    "# def read_transcript_head(json_path, num_segments=5):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     print(f\"\\n📄 First {num_segments} segments of: {json_path}\\n\")\n",
    "    \n",
    "#     for segment in data[:num_segments]:\n",
    "#         start = segment['start']\n",
    "#         end = segment['end']\n",
    "#         text = segment['text']\n",
    "#         print(f\"[{start} - {end}] {text}\")\n",
    "\n",
    "# read_transcript_head('/Users/test/Desktop/ironhack_labs/Final_Project_YouTube_Chatbot/transcripts/01 - Is The Universe Made of Tiny Vibrating Strings？ With Lara Anderson.en.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf92929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert json file into pandas df\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# def load_transcript_as_df(json_path):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "#     return df\n",
    "\n",
    "# # Example usage:\n",
    "# json_path = '/Users/test/Desktop/ironhack_labs/Final_Project_YouTube_Chatbot/transcripts/01 - Is The Universe Made of Tiny Vibrating Strings？ With Lara Anderson.en.json'\n",
    "# df = load_transcript_as_df(json_path)\n",
    "\n",
    "# # Display first few rows\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f21e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_all_transcripts_to_df(transcript_dir):\n",
    "#     all_data = []\n",
    "\n",
    "#     for filename in os.listdir(transcript_dir):\n",
    "#         if filename.endswith(\".json\"):\n",
    "#             json_path = os.path.join(transcript_dir, filename)\n",
    "#             with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#                 data = json.load(f)\n",
    "\n",
    "#             for entry in data:\n",
    "#                 entry_with_meta = entry.copy()\n",
    "#                 entry_with_meta['source_file'] = filename  # Add source file info\n",
    "#                 all_data.append(entry_with_meta)\n",
    "\n",
    "#     df = pd.DataFrame(all_data)\n",
    "#     return df\n",
    "\n",
    "# # Example usage\n",
    "# transcript_dir = '/Users/test/Desktop/ironhack_labs/Final_Project_YouTube_Chatbot/transcripts'\n",
    "# df_all = load_all_transcripts_to_df(transcript_dir)\n",
    "\n",
    "# # Display summary\n",
    "# # print(df_all.head())\n",
    "# print(df_all.head(5))\n",
    "# print(f\"\\nTotal segments loaded: {len(df_all)}\")\n",
    "# print(f\"Columns: {df_all.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b449ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
